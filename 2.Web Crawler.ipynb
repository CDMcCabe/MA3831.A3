{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import re\n",
    "import selenium.common.exceptions\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "\n",
    "# Get stories from creepypasta.com\n",
    "def creepy_url_get():\n",
    "    option = webdriver.FirefoxOptions()\n",
    "    option.add_argument(\"-incognito\")\n",
    "    option.add_argument(\"--headless\")\n",
    "\n",
    "    print(\"Getting text urls from creepypasta.com\")\n",
    "    browser = webdriver.Firefox(executable_path=\"webdriver/geckodriver.exe\", options=option)\n",
    "    browser.set_window_size(1920, 1080)\n",
    "\n",
    "    # Page load time\n",
    "    timeout = 20\n",
    "    \n",
    "    # Browser setup\n",
    "    browser.get(f'https://www.creepypasta.com/archive/top-ranked/?_orderby=_gdrts_stars-rating_rating%2Cdesc')\n",
    "    WebDriverWait(browser, timeout).until(ec.visibility_of_element_located((By.XPATH, f'//li/a[@title=\"Current page is 1\"]')))\n",
    "    \n",
    "    urls = []\n",
    "    \n",
    "    with open('urls.txt', 'a') as f:\n",
    "        for i in range(1, 151): # 151\n",
    "            if i % 10 == 0:\n",
    "                print(f'Page: {i}. {len(urls)} story links found.')\n",
    "            if i == 5:\n",
    "                print(f\"Got url:'{url}'\")\n",
    "            browser.get(f'https://www.creepypasta.com/archive/top-ranked/?_orderby=_gdrts_stars-rating_rating%2Cdesc&_page={i}')\n",
    "            WebDriverWait(browser, timeout).until(ec.visibility_of_element_located((By.XPATH, f'//li/a[@title=\"Current page is {i}\"]')))\n",
    "            try:\n",
    "            \n",
    "                # Hides iframe advertisements to improve speed and improve Selenium's functionality\n",
    "                iframes = browser.find_elements_by_tag_name(\"iframe\")\n",
    "                if len(iframes) > 0:\n",
    "                    browser.execute_script(\n",
    "                        \"\"\"\n",
    "                        var elems = document.getElementsByTagName(\"iframe\");\n",
    "                            for(var i = 0, max = elems.length; i < max; i++){\n",
    "                            elems[i].hidden=true;\n",
    "                        }\n",
    "                        var ads = document.getElementsByClassName(\"ad-tag\");\n",
    "                        for(var i =0, max = ads.length; i < max; i++){\n",
    "                            ads[i].hidden=true;\n",
    "                        }\n",
    "                        var ads = document.getElementsByClassName(\"ad-sticky\");\n",
    "                        for(var i =0, max = ads.length; i < max; i++){\n",
    "                            ads[i].hidden=true;\n",
    "                        }\n",
    "                        var ads = document.getElementsByClassName(\"ad-sticky-desktop-anchor\");\n",
    "                        for(var i =0, max = ads.length; i < max; i++){\n",
    "                            ads[i].hidden=true;\n",
    "                        }\n",
    "                        \"\"\")\n",
    "                texts = []\n",
    "            \n",
    "                # Finds all urls that point to stories based on class identifier\n",
    "                texts = browser.find_elements_by_xpath(\"//a[@class='_self cvplbd']\")\n",
    "                for text in texts:\n",
    "                    try:\n",
    "                        url = text.get_attribute('href')\n",
    "                        f.write(url+'\\n')\n",
    "                        urls.append(url)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error 1: {e}\")\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error 2: {e}\")\n",
    "                break\n",
    "            browser.find_element_by_xpath(f'//li/a[@title=\"Go to page {i+1}\"]').click()\n",
    "        time.sleep(1)\n",
    "    browser.quit()\n",
    "    f.close()\n",
    "    print(f'{len(urls)} urls saved.')\n",
    "    \n",
    "    \n",
    "def creepy_requests(urls):\n",
    "    print(\"Downloading texts from creepypasta.com\")\n",
    "    creepy_stories = []\n",
    "    j = 1\n",
    "    for url in urls:\n",
    "        if j % 250 == 0:\n",
    "            print(f'{j}/{len(urls)}')\n",
    "        r = requests.get(url)\n",
    "        if r.ok:                \n",
    "            # bs4 parsing\n",
    "            soup = bs(r.content, 'html.parser')\n",
    "                        \n",
    "            text = []\n",
    "            paras = soup.find_all('p', href_= None, id_= None)\n",
    "            for sente in paras:\n",
    "                sente = sente.get_text()\n",
    "                sente = sente.replace(u'\\xa0', u' ')\n",
    "                breaks = [\"Author's Note: \", \"Advertisements\", \"-Charles \", \"STORY BY \", \"Credit To\", \"Credit :\", \"Credit: \", \"CREDIT:\", \"CREDIT :\", \"Written \", \"WRITTEN \", \"Thanks Again, \", \"The author of this story wrote it for free.\"]\n",
    "                if not any(word in sente for word in breaks):\n",
    "                    text.append(sente.strip())\n",
    "                else:\n",
    "                    break\n",
    "                story = ' '.join(text)\n",
    "            creepy_stories.append(story.strip())\n",
    "            time.sleep(0.3)\n",
    "        j += 1\n",
    "    df = pd.DataFrame(creepy_stories, columns=['stories'])\n",
    "    df.to_csv(\"stories.csv\")\n",
    "    print(f'{len(creepy_stories)} stories downloaded.')\n",
    "\n",
    "    \n",
    "# Get urls\n",
    "creepy_url_get()\n",
    "\n",
    "# Get texts and save to csv\n",
    "with open('urls.txt', 'r') as f:\n",
    "    urls = f.read().splitlines()\n",
    "    creepy_requests(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
